{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Path to local model directory\n",
    "model_path = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Timer: Model loading\n",
    "start_time = time.time()\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer_start = time.time()\n",
    "\n",
    "# Load the tokenizer from local directory\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path, do_lower_case=False)\n",
    "\n",
    "tokenizer_time = time.time() - tokenizer_start\n",
    "print(f\"✓ Tokenizer loaded in {tokenizer_time:.2f}s\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model_start = time.time()\n",
    "\n",
    "# Load the model from local directory\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "model_load_time = time.time() - model_start\n",
    "print(f\"✓ Model loaded in {model_load_time:.2f}s\")\n",
    "\n",
    "print(\"Converting model precision...\")\n",
    "precision_start = time.time()\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.float() if device.type=='cpu' else model.half()\n",
    "\n",
    "precision_time = time.time() - precision_start\n",
    "total_setup_time = time.time() - start_time\n",
    "print(f\"✓ Model precision converted in {precision_time:.2f}s\")\n",
    "print(f\"✓ Total setup time: {total_setup_time:.2f}s\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# prepare your protein sequences/structures as a list.\n",
    "# Amino acid sequences are expected to be upper-case (\"PRTEINO\" below)\n",
    "# while 3Di-sequences need to be lower-case.\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "min_len = min([ len(s) for s in sequence_examples])\n",
    "max_len = max([ len(s) for s in sequence_examples])\n",
    "\n",
    "print(\"Data preparation...\")\n",
    "prep_start = time.time()\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from AAs to 3Di, you need to prepend \"<AA2fold>\"\n",
    "sequence_examples = [ \"<AA2fold>\" + \" \" + s for s in sequence_examples]\n",
    "\n",
    "print(f\"Processing sequences: {sequence_examples}\")\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "prep_time = time.time() - prep_start\n",
    "print(f\"✓ Data preparation completed in {prep_time:.3f}s\")\n",
    "\n",
    "# Generation configuration for \"folding\" (AA-->3Di)\n",
    "gen_kwargs_aa2fold = {\n",
    "                  \"do_sample\": True,\n",
    "                  \"num_beams\": 3, \n",
    "                  \"top_p\" : 0.95, \n",
    "                  \"temperature\" : 1.2, \n",
    "                  \"top_k\" : 6,\n",
    "                  \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "print(\"Translating AA to 3Di...\")\n",
    "# translate from AA to 3Di (AA-->3Di)\n",
    "with torch.no_grad():\n",
    "      translations = model.generate( \n",
    "              ids.input_ids, \n",
    "              attention_mask=ids.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              early_stopping=True, # stop early if end-of-text token is generated\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_aa2fold\n",
    "  )\n",
    "\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_translations = tokenizer.batch_decode( translations, skip_special_tokens=True )\n",
    "structure_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_translations ] # predicted 3Di strings\n",
    "\n",
    "print(f\"3Di sequences: {structure_sequences}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3Di → AA BACK-TRANSLATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Timer: Data preparation for back-translation\n",
    "backtrans_prep_start = time.time()\n",
    "\n",
    "# Now we can use the same model and invert the translation logic\n",
    "# to generate an amino acid sequence from the predicted 3Di-sequence (3Di-->AA)\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from 3Di to AA (3Di-->AA), you need to prepend \"<fold2AA>\"\n",
    "sequence_examples_backtranslation = [ \"<fold2AA>\" + \" \" + s for s in decoded_translations]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids_backtranslation = tokenizer.batch_encode_plus(sequence_examples_backtranslation,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "backtrans_prep_time = time.time() - backtrans_prep_start\n",
    "print(f\"✓ Back-translation data prep: {backtrans_prep_time:.3f}s\")\n",
    "\n",
    "# Example generation configuration for \"inverse folding\" (3Di-->AA)\n",
    "gen_kwargs_fold2AA = {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\" : 0.85,\n",
    "            \"temperature\" : 1.0,\n",
    "            \"top_k\" : 3,\n",
    "            \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# Timer: 3Di to AA translation\n",
    "fold2aa_start = time.time()\n",
    "print(\"Translating 3Di back to AA...\")\n",
    "\n",
    "# translate from 3Di to AA (3Di-->AA)\n",
    "with torch.no_grad():\n",
    "      backtranslations = model.generate( \n",
    "              ids_backtranslation.input_ids, \n",
    "              attention_mask=ids_backtranslation.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              #early_stopping=True, # stop early if end-of-text token is generated; only needed for beam-search\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_fold2AA\n",
    ")\n",
    "\n",
    "fold2aa_generation_time = time.time() - fold2aa_start\n",
    "\n",
    "# Timer: Final decoding\n",
    "final_decode_start = time.time()\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_backtranslations = tokenizer.batch_decode( backtranslations, skip_special_tokens=True )\n",
    "aminoAcid_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_backtranslations ]\n",
    "final_decode_time = time.time() - final_decode_start\n",
    "\n",
    "total_fold2aa_time = time.time() - fold2aa_start\n",
    "print(f\"✓ 3Di→AA generation: {fold2aa_generation_time:.2f}s\")\n",
    "print(f\"✓ Final decoding: {final_decode_time:.3f}s\")\n",
    "print(f\"✓ Total 3Di→AA time: {total_fold2aa_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original AA sequences: {['PRTEINO', 'SEQWENCE']}\")\n",
    "print(f\"Predicted 3Di sequences: {structure_sequences}\")\n",
    "print(f\"Back-translated AA sequences: {aminoAcid_sequences}\")\n",
    "\n",
    "# Calculate and display total runtime\n",
    "total_runtime = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TIMING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Setup time:           {total_setup_time:.2f}s\")\n",
    "print(f\"  - Tokenizer:        {tokenizer_time:.2f}s\")\n",
    "print(f\"  - Model loading:    {model_load_time:.2f}s\")\n",
    "print(f\"  - Precision conv:   {precision_time:.2f}s\")\n",
    "print(f\"Data preparation:     {prep_time:.3f}s\")\n",
    "print(f\"AA→3Di translation:   {total_aa2fold_time:.2f}s\")\n",
    "print(f\"  - Generation:       {aa2fold_generation_time:.2f}s\")\n",
    "print(f\"  - Decoding:         {decode_time:.3f}s\")\n",
    "print(f\"Back-translation:     {total_fold2aa_time:.2f}s\")\n",
    "print(f\"  - Data prep:        {backtrans_prep_time:.3f}s\")\n",
    "print(f\"  - Generation:       {fold2aa_generation_time:.2f}s\")\n",
    "print(f\"  - Decoding:         {final_decode_time:.3f}s\")\n",
    "print(f\"TOTAL RUNTIME:        {total_runtime:.2f}s\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
