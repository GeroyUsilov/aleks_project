{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65165a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import re\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Rostlab/ProstT5\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.float() if device.type=='cpu' else model.half()\n",
    "\n",
    "# prepare your protein sequences/structures as a list.\n",
    "# Amino acid sequences are expected to be upper-case (\"PRTEINO\" below)\n",
    "# while 3Di-sequences need to be lower-case.\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "min_len = min([ len(s) for s in sequence_examples])\n",
    "max_len = max([ len(s) for s in sequence_examples])\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from AAs to 3Di, you need to prepend \"<AA2fold>\"\n",
    "sequence_examples = [ \"<AA2fold>\" + \" \" + s for s in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Generation configuration for \"folding\" (AA-->3Di)\n",
    "gen_kwargs_aa2fold = {\n",
    "                  \"do_sample\": True,\n",
    "                  \"num_beams\": 3, \n",
    "                  \"top_p\" : 0.95, \n",
    "                  \"temperature\" : 1.2, \n",
    "                  \"top_k\" : 6,\n",
    "                  \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from AA to 3Di (AA-->3Di)\n",
    "with torch.no_grad():\n",
    "      translations = model.generate( \n",
    "              ids.input_ids, \n",
    "              attention_mask=ids.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              early_stopping=True, # stop early if end-of-text token is generated\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_aa2fold\n",
    "  )\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_translations = tokenizer.batch_decode( translations, skip_special_tokens=True )\n",
    "structure_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_translations ] # predicted 3Di strings\n",
    "\n",
    "# Now we can use the same model and invert the translation logic\n",
    "# to generate an amino acid sequence from the predicted 3Di-sequence (3Di-->AA)\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from 3Di to AA (3Di-->AA), you need to prepend \"<fold2AA>\"\n",
    "sequence_examples_backtranslation = [ \"<fold2AA>\" + \" \" + s for s in decoded_translations]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids_backtranslation = tokenizer.batch_encode_plus(sequence_examples_backtranslation,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Example generation configuration for \"inverse folding\" (3Di-->AA)\n",
    "gen_kwargs_fold2AA = {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\" : 0.85,\n",
    "            \"temperature\" : 1.0,\n",
    "            \"top_k\" : 3,\n",
    "            \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from 3Di to AA (3Di-->AA)\n",
    "with torch.no_grad():\n",
    "      backtranslations = model.generate( \n",
    "              ids_backtranslation.input_ids, \n",
    "              attention_mask=ids_backtranslation.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              #early_stopping=True, # stop early if end-of-text token is generated; only needed for beam-search\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_fold2AA\n",
    ")\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_backtranslations = tokenizer.batch_decode( backtranslations, skip_special_tokens=True )\n",
    "aminoAcid_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_backtranslations ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2667e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
